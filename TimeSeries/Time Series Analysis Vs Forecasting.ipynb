{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0877a2e7",
   "metadata": {},
   "source": [
    "<b> Time Series Study </b>\n",
    "\n",
    "A time series is a sequence of observations taken sequentially in time.\n",
    "\n",
    "Time series adds an explicit order dependence between observations: a time dimension.\n",
    "\n",
    "This additional dimension is both a constraint and a structure that provides a source of additional information.\n",
    "\n",
    "The purpose of time series analysis is generally twofold: \n",
    "1. To understand or model the stochastic mechanisms that gives rise to an observed series and \n",
    "2. To predict or forecast the future values of a series based on the history of that series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6846d",
   "metadata": {},
   "source": [
    "<b> 1. Time Series Analysis </b>\n",
    "\n",
    "<b> The primary objective of time series analysis is to develop mathematical models that provide plausible descriptions from sample data </b>\n",
    "\n",
    "Time series analysis involves developing models that best capture or describe an observed time series in order to understand the underlying causes. This field of study seeks the <b> “why” </b> behind a time series dataset.\n",
    "\n",
    "This often involves making assumptions about the form of the data and decomposing the time series into constitution components.\n",
    "\n",
    "The quality of a descriptive model is determined by how well it describes all available data and the interpretation it provides to better inform the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b27f0a",
   "metadata": {},
   "source": [
    "<b> Different Time Series Analysis & Decomposition Techniques </b>\n",
    "\n",
    "<b> 1. Autocorrelation Analysis:</b> A statistical method to measure the correlation between a time series and a lagged version of itself at different time lags. It helps identify patterns and dependencies within the time series data.\n",
    "\n",
    "<b> 2. Partial Autocorrelation Functions (PACF):</b> PACF measures the correlation between a time series and its lagged values, controlling for intermediate lags, aiding in identifying direct relationships between variables.\n",
    "\n",
    "<b> 3. Trend Analysis:</b> The process of identifying and analyzing the long-term movement or directionality of a time series. Trends can be linear, exponential, or nonlinear and are crucial for understanding underlying patterns and making forecasts.\n",
    "\n",
    "<b> 4. Seasonality Analysis:</b> Seasonality refers to periodic fluctuations or patterns that occur in a time series at fixed intervals, such as daily, weekly, or yearly. Seasonality analysis involves identifying and quantifying these recurring patterns to understand their impact on the data.\n",
    "\n",
    "<b> 5. Decomposition:</b> Decomposition separates a time series into its constituent components, typically trend, seasonality, and residual (error). This technique helps isolate and analyze each component individually, making it easier to understand and model the underlying patterns.\n",
    "\n",
    "<b> 6. Spectrum Analysis:</b> Spectrum analysis involves examining the frequency domain representation of a time series to identify dominant frequencies or periodicities. It helps detect cyclic patterns and understand the underlying periodic behavior of the data.\n",
    "\n",
    "<b> 7. Seasonal and Trend decomposition using Loess:</b> STL decomposes a time series into three components: seasonal, trend, and residual. This decomposition enables modeling and forecasting each component separately, simplifying the forecasting process.\n",
    "\n",
    "<b> 8. Rolling Correlation:</b> Rolling correlation calculates the correlation coefficient between two time series over a rolling window of observations, capturing changes in the relationship between variables over time.\n",
    "\n",
    "<b> 9. Cross-correlation Analysis:</b> Cross-correlation analysis measures the similarity between two time series by computing their correlation at different time lags. It is used to identify relationships and dependencies between different variables or time series.\n",
    "\n",
    "<b> 10. Box-Jenkins Method:</b> Box-Jenkins Method is a systematic approach for analyzing and modeling time series data. It involves identifying the appropriate autoregressive integrated moving average (ARIMA) model parameters, estimating the model, diagnosing its adequacy through residual analysis, and selecting the best-fitting model.\n",
    "\n",
    "<b> 11.Granger Causality Analysis:</b> Granger causality analysis determines whether one time series can predict future values of another time series. It helps infer causal relationships between variables in time series data, providing insights into the direction of influence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0791b4d",
   "metadata": {},
   "source": [
    "<b> 2. Time Series Forecasting </b>\n",
    "\n",
    "<b> Making predictions about the future is called extrapolation in the classical statistical handling of time series data and and referred as time series forecasting. </b>\n",
    "\n",
    "Forecasting involves taking models fit on historical data and using them to predict future observations. \n",
    "\n",
    "An important distinction in forecasting is that the future is completely unavailable and must only be estimated from what has already happened.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558907fd",
   "metadata": {},
   "source": [
    "<b> Components of Time Series </b>\n",
    "Time series analysis provides a body of techniques to better understand a dataset.\n",
    "\n",
    "4 constituent parts of time series are\n",
    "1. Level - The baseline value for the series if it were a straight line.\n",
    "2. Trend - The optional and often linear increasing or decreasing behavior of the series over time.\n",
    "3. Seasonality - The optional repeating patterns or cycles of behavior over time.\n",
    "4. Noise - The optional variability in the observations that cannot be explained by the model.\n",
    "\n",
    "y = level + trend + seasonality + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43694305",
   "metadata": {},
   "source": [
    "1. <b> How much data do you have available and are you able to gather it all together? </b>\n",
    "More data is often more helpful, offering greater opportunity for exploratory data analysis, model testing and tuning, and model fidelity.\n",
    "2. <b> What is the time horizon of predictions that is required? Short, medium or long term? </b>\n",
    "Shorter time horizons are often easier to predict with higher confidence.\n",
    "3. <b> Can forecasts be updated frequently over time or must they be made once and remain static? </b>\n",
    "Updating forecasts as new information becomes available often results in more accurate predictions.\n",
    "4. <b> At what temporal frequency are forecasts required? </b>\n",
    "Often forecasts can be made at a lower or higher frequencies, allowing you to harness down-sampling, and up-sampling of data, which in turn can offer benefits while modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b46520",
   "metadata": {},
   "source": [
    "<b> 11 Classical time series forecasting methods </b>\n",
    "1. Autoregression (AR)\n",
    "2. Moving Average (MA)\n",
    "3. Autoregressive Moving Average (ARMA)\n",
    "4. Autoregressive Integrated Moving Average (ARIMA)\n",
    "5. Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
    "6. Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX)\n",
    "7. Vector Autoregression (VAR)\n",
    "8. Vector Autoregression Moving-Average (VARMA)\n",
    "9. Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX)\n",
    "10. Simple Exponential Smoothing (SES)\n",
    "11. Holt Winter’s Exponential Smoothing (HWES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175b94c",
   "metadata": {},
   "source": [
    "<b> 1. Autoregression (AR) </b>\n",
    "\n",
    "The autoregression (AR) method predicts the subsequent value in a sequence using a linear combination of previous observations.\n",
    "Not Recommended\n",
    "\n",
    "<b> 2. Moving Average (MA) </b>\n",
    "\n",
    "The Moving Average (MA) method models predict the next step in the sequence as a linear function of the residual errors from a mean process at prior time steps. It’s important to note that a Moving Average model is different from calculating the moving average of the time series.The method is suitable for univariate time series without trend and seasonal components.\n",
    "Not Recommended\n",
    "\n",
    "<b> 3. Autoregressive Moving Average (ARMA) </b>\n",
    "\n",
    "The Autoregressive Moving Average (ARMA) method model predicts the next step in the sequence based on a linear combination of both past observations and past residual errors. \n",
    "\n",
    "The method combines both Autoregression (AR) and Moving Average (MA) models.\n",
    "Not Recommended\n",
    "\n",
    " <b> 4. Autoregressive Integrated Moving Average (ARIMA) </b>\n",
    "\n",
    "The Autoregressive Integrated Moving Average (ARIMA) method model predicts the next step in the sequence as a linear function of the differenced observations and residual errors at prior time steps.\n",
    "\n",
    "The method integrates the principles of Autoregression (AR) and Moving Average (MA) models as well as a differencing pre-processing step of the sequence to make the sequence stationary, called integration (I). The ARIMA approach is optimal for single-variable time series that exhibit a trend but lack seasonal variations.\n",
    "\n",
    "<b> 5. Seasonal Autoregressive Integrated Moving-Average (SARIMA) </b>\n",
    "\n",
    "The Seasonal Autoregressive Integrated Moving Average (SARIMA) method models the next step in the sequence based on a linear blend of differenced observations, errors, differenced seasonal observations, and seasonal errors at prior time steps.\n",
    "\n",
    "SARIMA enhances the ARIMA model with the ability to perform the same autoregression, differencing, and moving average modeling at the seasonal level.\n",
    "A SARIMA model can be used to develop AR, MA, ARMA and ARIMA models. The method is suitable for univariate time series with trend and/or seasonal components.\n",
    "\n",
    "<b> 6. Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) </b>\n",
    "The Seasonal Autoregressive Integrated Moving-Average with Exogenous Regressors (SARIMAX) is an extension of the SARIMA model that also includes the modeling of exogenous variables.The method is suitable for univariate time series with trend and/or seasonal components and exogenous variables.\n",
    "\n",
    "<b> 7. Vector Autoregression (VAR) </b>\n",
    "The Vector Autoregression (VAR) method models the next step in each time series using an AR model approach. Essentially, it extends the AR model to cater to multiple parallel time series, e.g. multivariate time series.\n",
    "The method is suitable for multivariate time series without trend and seasonal components.\n",
    "\n",
    "<b> 8. Vector Autoregression Moving-Average (VARMA) </b>\n",
    "The Vector Autoregression Moving-Average (VARMA) method models the upcoming value in multiple time series by utilising the ARMA model approach. It is the generalization of ARMA to multiple parallel time series, e.g. multivariate time series.The method is suitable for multivariate time series without trend and seasonal components.\n",
    "\n",
    "<b> 9. Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX) </b>\n",
    "The Vector Autoregression Moving-Average with Exogenous Regressors (VARMAX) extends the capabilities of the VARMA model which also includes the modelling of exogenous variables. It is a multivariate version of the ARMAX method.\n",
    "Exogenous variables, also called covariates and can be thought of as parallel input sequences that align with the time steps as the original series. The primary series(es) are referred to as endogenous data to contrast it from the exogenous sequence(s). The observations for exogenous variables are included in the model directly at each time step and are not modeled in the same way as the primary endogenous sequence (e.g. as an AR, MA, etc. process).\n",
    "The VARMAX method can also be used to model the subsumed models with exogenous variables, such as VARX and VMAX.\n",
    "\n",
    "The method is suitable for multivariate time series without trend and seasonal components with exogenous variables.\n",
    "\n",
    "<b> 10. Simple Exponential Smoothing (SES) </b>\n",
    "The Simple Exponential Smoothing (SES) method models the next time step as an exponentially weighted linear function of observations at prior time steps.\n",
    "\n",
    "The method is suitable for univariate time series without trend and seasonal components.\n",
    "\n",
    "<b> 11. Holt Winter’s Exponential Smoothing (HWES) </b>\n",
    "The Holt Winter’s Exponential Smoothing (HWES) also called the Triple Exponential Smoothing method models the next time step as an exponentially weighted linear function of observations at prior time steps, taking trends and seasonality into account.\n",
    "\n",
    "The method is suitable for univariate time series with trend and/or seasonal components.\n",
    "\n",
    "<b> Specifically, the stats library in Python has tools for building ARMA models, ARIMA models and SARIMA models with just a few lines of code.</b>\n",
    "\n",
    "<b> 12. Theta Method: </b> A simple and intuitive forecasting technique based on extrapolation and trend fitting.\n",
    "\n",
    "<b> 13. Random Forests: </b> Random Forests is a machine learning ensemble method that constructs multiple decision trees during training and outputs the average prediction of the individual trees. It can handle complex relationships and interactions in the data, making it effective for time series forecasting.\n",
    "\n",
    "<b> 14. Gradient Boosting Machines (GBM): </b> GBM is another ensemble learning technique that builds multiple decision trees sequentially, where each tree corrects the errors of the previous one. It excels in capturing nonlinear relationships and is robust against overfitting.\n",
    "\n",
    "<b> 15. Hidden Markov Model (HMM): </b> A Hidden Markov Model (HMM) is a statistical model used to describe sequences of observable events generated by underlying hidden states. In time series, HMMs infer hidden states from observed data, capturing dependencies and transitions between states. They are valuable for tasks like speech recognition, gesture analysis, and anomaly detection, providing a framework to model complex sequential data and extract meaningful patterns from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441a094",
   "metadata": {},
   "source": [
    "<b> Preprocessing Time Series Data </b>\n",
    "\n",
    "<b> Handling Missing Values:</b>  Dealing with missing values in the time series data to ensure continuity and reliability in analysis.\n",
    "\n",
    "<b> Dealing with Outliers: </b> Identifying and addressing observations that significantly deviate from the rest of the data, which can distort analysis results.\n",
    "\n",
    "<b> Stationarity and Transformation:</b>  Ensuring that the statistical properties of the time series, such as mean and variance, remain constant over time. Techniques like differencing, detrending, and deseasonalizing are used to achieve stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656027d",
   "metadata": {},
   "source": [
    "<b> Performance Metrics: </b> \n",
    "Performance metrics are quantitative measures used to evaluate the accuracy and effectiveness of time series forecasts. These metrics provide insights into how well a forecasting model performs in predicting future values based on historical data. \n",
    "\n",
    "Common performance metrics which can be used for time series include:\n",
    "\n",
    "<b> Mean Absolute Error (MAE): </b>  Measures the average magnitude of errors between predicted and actual values.\n",
    "\n",
    "<b> Mean Absolute Percentage Error (MAPE): </b> Calculates the average percentage difference between predicted and actual values.\n",
    "\n",
    "<b> Mean Squared Error (MSE): </b> Computes the average squared differences between predicted and actual values.\n",
    "\n",
    "<b> Root Mean Squared Error (RMSE):</b> The square root of MSE, providing a measure of the typical magnitude of errors.\n",
    "\n",
    "<b> Forecast Bias: </b> Determines whether forecasts systematically overestimate or underestimate actual values.\n",
    "\n",
    "<b> Forecast Interval Coverage:</b>  Evaluates the percentage of actual values that fall within forecast intervals.\n",
    "\n",
    "<b> Theil's U Statistic:</b> Compares the performance of the forecast model to a naïve benchmark model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dc0df2",
   "metadata": {},
   "source": [
    "<b> Cross-Validation Techniques </b>\n",
    "Cross-validation techniques are used to assess the generalization performance of time series forecasting models. \n",
    "These techniques involve splitting the available data into training and testing sets, fitting the model on the training data, \n",
    "and evaluating its performance on the unseen testing data. Common cross-validation techniques for time series data include:\n",
    "\n",
    "<b> 1. Train-Test Split for Time Series: </b> Divides the dataset into a training set for model fitting and a separate testing set for evaluation.\n",
    "\n",
    "<b> 2. Rolling Window Validation: </b> Uses a moving window approach to iteratively train and test the model on different subsets of the data.\n",
    "\n",
    "<b> 3. Time Series Cross-Validation: </b> Splits the time series data into multiple folds, ensuring that each fold maintains the temporal order of observations.\n",
    "\n",
    "<b> 4. Walk-Forward Validation: </b> Similar to rolling window validation but updates the training set with each new observation, allowing the model to adapt to changing data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c90421",
   "metadata": {},
   "source": [
    "<b> Dynamic Time Warping (DTW) </b> is a technique used to measure the similarity between two sequences of data that may vary in time or speed. It aligns the sequences by stretching or compressing them in time to find the optimal matching between corresponding points.\n",
    "DTW is commonly used in time series analysis, speech recognition, and pattern recognition tasks where the sequences being compared have different lengths or rates of change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
