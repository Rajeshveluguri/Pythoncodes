{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a8Qbuo_e4s3"
   },
   "source": [
    "<center><img src=\"https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true\" width=\"240\" height=\"100\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqPj8IubYBdx"
   },
   "source": [
    "<center><h1>Introduction to Language Modelling</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dDvFeJzYKeL"
   },
   "source": [
    "---\n",
    "# **Table of Contents**\n",
    "---\n",
    "\n",
    "**1.** [**Introduction**](#Section1)<br>\n",
    "\n",
    "**2.** [**Statistical Language Models**](#Section2)<br>\n",
    "  - **2.1** [**Character Based**](#Section21)\n",
    "  - **2.2** [**N-Gram Model**](#Section22)\n",
    "  - **2.3** [**Limitation of Statistical Approach**](#Section23)\n",
    "\n",
    "**3.** [**Neural Language Model**](#Section3)<br>\n",
    "**4.** [**Generalised Language Model**](#Section4)<br>\n",
    "**5.** [**Application of Language Model**](#Section5)<br>\n",
    "**6.** [**Conclusion**](#Section6)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lt3g_Q2dZJwN"
   },
   "source": [
    "---\n",
    "<a name = Section1></a>\n",
    "# **1. Introduction**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D-mJwBSKUaA"
   },
   "source": [
    "- **Language modeling** is the use of various **statistical** and **probabilistic** techniques to determine the **probability** of a given sequence of words occurring in a sentence. \n",
    "\n",
    "- Language models **analyze** bodies of text data to **provide** a basis for their **word predictions**. They are used in natural language processing (NLP) applications, particularly ones that **generate** text as an **output**. \n",
    "\n",
    "  - For example, a language model used for **predicting** the next word in a **search query** will be absolutely different from those used in **predicting** the next word in a **long document** (such as Google Docs).\n",
    "\n",
    "  - The approach followed to **train** the model would be **unique** in both cases.\n",
    "\n",
    "- Language Models determine the **probability** of the **next** word by **analyzing** the text in data. These models **interpret** the data by **feeding** it through algorithms. \n",
    "\n",
    "- For **training** a language model, a number of **probabilistic** approaches are used.\n",
    "\n",
    "- These approaches **vary** on the basis of **purpose** for which a **language** model is created.\n",
    "\n",
    "- Besides assigning a **probability** to each **sequence** of words, the **language models** also assigns a **probability** for the likelihood of a given word to **follow** a **sequence** of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6s335W0rWyLA"
   },
   "source": [
    "\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/lm10.gif\"/></center>\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0GJh6ZThBig"
   },
   "source": [
    "- **Language modelling** by itself does not have a **direct** practical use but it is a **crucial** component in **real-world applications** such as **machine-translation** and **automatic speech recognition**.\n",
    "\n",
    "- A translation system might generate **multiple** **translations** of the same target sentence and the **language models** scores all the sentences to pick the one that is most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnRpU8WSZIze"
   },
   "source": [
    "---\n",
    "<a name = Section2></a>\n",
    "# **2. Statistical Language Models**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLsEcOLrM4kH"
   },
   "source": [
    "- **Statistical models** include the development of **probabilistic models** that are able to predict the **next word** in the sequence, given the words that **precede** it.\n",
    "\n",
    "- A number of **statistical language models** are in use already. Some of them are as follows:\n",
    "\n",
    "  - **The count-based methods**, such as traditional **statistical models**, usually involve making an n-th order **Markov assumption** and estimating **N-gram** probabilities via **counting** and subsequent **smoothing**. \n",
    "\n",
    "  - Using a **statistical** formulation to describe a **Language Modelling** is to construct the joint **probability** distribution of a sequence of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQVWkoJ1ZpV6"
   },
   "source": [
    "<a name = Section21></a>\n",
    "### **2.1 Character Based**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Or8bQsMAG"
   },
   "source": [
    "- It predicts **next character** based on **previous characters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko-XWOOTs7V8"
   },
   "source": [
    "<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/tughtu.JPG\"height = \"300\"Width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1DS19kWZpSi"
   },
   "source": [
    "<a name = Section22></a>\n",
    "### **2.2 N-Gram Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3akFr3m0NfcY"
   },
   "source": [
    "- In **N-Gram Model**, the process of **predicting** a word sequence is **broken** up into predicting one word at a time.\n",
    "\n",
    "- The **LM probability** *p(w1,w2,…,wn)* is a product of word **probabilities** based on a history of **preceding** words, whereby the history is limited to **m** words:\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/lm3.png\"height = \"60\"Width=600\"/></center>\n",
    "\n",
    "<br>  \n",
    "\n",
    "- This is also called a **Markov chain**, where the number of **previous** states is the **order** of the model.\n",
    "\n",
    "- The basic idea for **n-gram LM** is that we can predict the **probability** of w_(n+1) with its **preceding context**, by dividing the **number** of **occurrences** of w_n, w_(n+1) by the number of **occurrences** of w_n, which then would be called a **bigram**. \n",
    "\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/lm4.png\"height = \"50\"Width=\"300\"/></center>\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRPocn2g0Ccz"
   },
   "source": [
    "## Unigram\n",
    "- An n-gram of **size 1** is referred to as a \"unigram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXNgLaygzHe3"
   },
   "source": [
    "<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/unigram.JPG\" height = \"100\"Width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqFwK2PP0FEt"
   },
   "source": [
    "## Bigram\n",
    "- An n-gram of **size 2** is referred to as a \"bigram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elPfrnX_zmhB"
   },
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/bigram.JPG\" height = \"100\"Width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LpcJXa20Hhn"
   },
   "source": [
    "## Trigram \n",
    "-  An n-gram of **size 3** is referred to as a \"trigram\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocbzM3KGz7zZ"
   },
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/trigram.JPG\" height = \"100\"Width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjYiH0ZAGRnM"
   },
   "source": [
    "## Unigram , Bigram , Trigram altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXP6adsuzAnN"
   },
   "source": [
    "<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/language.JPG\" height = \"300\"Width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8IQAMWSfWei"
   },
   "source": [
    "-  The **higher** the N, the **better** is the model **usually**. But this leads to lots of **computation** overhead that requires large **computation power** in terms of RAM.\n",
    "\n",
    "- N-grams are a **sparse representation** of language. This is because we build the **model** based on the **probability** of words **co-occurring**. It will give **zero probability** to all the words that are **not present** in the training corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_rXgzgiN_aj"
   },
   "source": [
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/lm15.png\"height = \"300\"Width=\"700\"/></center>\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5EK7GfVMI8g"
   },
   "source": [
    "#### **Bidirectional**:\n",
    "\n",
    "- Unlike N-gram models, which **analyze** text in **one direction** (backwards), **bidirectional** models analyze text in **both** directions, backwards and forwards. \n",
    "\n",
    "- These models can **predict** any word in a **sentence** or **body** of text by using every **other** word in the text. Examining text **bidirectionally** increases result **accuracy**. \n",
    "\n",
    "- This type is often **utilized** in machine learning and **speech generation** applications. For example, **Google** uses a **bidirectional model** to process search **queries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z3vW6YFZpPk"
   },
   "source": [
    "<a name = Section23></a>\n",
    "### **2.3 Limitation of Statistical Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7R0zFsUNe1S"
   },
   "source": [
    "- The **distributed representation** approach allows the **embedding** representation to **scale** better with the size of the vocabulary. \n",
    "\n",
    "- **Classical methods** that have one **discrete** representation per word fight the **curse of dimensionality** with **larger vocabularies** of words that result in **longer** and more **sparse** representations.\n",
    "\n",
    "- Despite the **smoothing** techniques, and the **practical usability** of n-gram , the **curse of dimensionality** is especially potent here, as there is a **huge number** of different **combinations** of values of the **input** variables that must be **discriminated** from each other. \n",
    "\n",
    "- For LM, this is the huge **number** of possible **sequences** of words, e.g., with a **sequence** of **10 words** taken from a vocabulary of **100,000**, there are **10⁵⁰ possible** sequences.\n",
    "\n",
    "- **N-grams** are a **sparse representation** of language. This is because we build the **model** based on the **probability** of words **co-occurring**. It will give **zero probability** to all the words that are **not present** in the **training** corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WmTN1xLZIpz"
   },
   "source": [
    "---\n",
    "<a name = Section3></a>\n",
    "# **3. Neural Language Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7krflWjqNAsp"
   },
   "source": [
    "- These language models are based on **neural networks** and are often considered as an **advanced approach** to execute NLP tasks. \n",
    "\n",
    "- There are **two** main NLM: \n",
    "  \n",
    "  - **Feed-forward neural network based LM**\n",
    "  \n",
    "  - **Recurrent neural network based LM**\n",
    "\n",
    "- **Neural Language Models** (NLM) address the **N-gram data sparsity** issue through parameterization of words as vectors (**word embeddings**) and using them as **inputs** to a neural network. \n",
    "\n",
    "- **Word embeddings** obtained through NLMs **exhibit** the property whereby **semantically** close words are **likewise close** in the induced **vector space**.\n",
    "\n",
    "- This learned **representation** of words based on their **usage** allows words with a **similar meaning** to have a similar representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaaGs2Z5W4K-"
   },
   "source": [
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/lm8.png\"width=\"690\" height=\"350\"/></center>\n",
    "\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NcJbURKjRZe"
   },
   "source": [
    "### Feed-Forward Neural Network Based Models\n",
    "\n",
    "- The first neural approach to LM is a **neural probabilistic language model**.\n",
    "\n",
    "-  This learns the **parameters** of **conditional probability distribution** of the next word, given the previous n-1 words using a **feed-forward neural** network of three layers. An overview of the **network** architecture is **additionally** given in following figure:\n",
    "\n",
    "- Bulid a **mapping** C from each **word** i of the **vocabulary** V to a distributed, **real-valued feature vector** C(i) ∈R^m, with m being the number of **features**. C is a |V| × m **matrix**, whose **row** i is the **feature** vector C(i) for **word** i.\n",
    "\n",
    "- Learn the word **feature vectors** and the parameters of that probability function with a composite **function f**, comprised of the **two mappings C and g**.\n",
    "\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/lm5.png\"width=\"490\" height=\"350\"/></center>\n",
    "\n",
    "<br>  \n",
    "\n",
    "- In this model, each word in the **vocabulary** is associated with a **distributed word** feature vector, and the joint **probability function** of words sequence is expressed by a **function** of the **feature vectors** of these words in the sequence.\n",
    "\n",
    "- The model can learn the word **feature vectors** and the parameters of that **probability function** simultaneously.\n",
    "\n",
    "- This **neural network** approach can solve the **sparseness problem**, and have also been shown to **generalize** well in comparison to the n-gram models in terms of perplexity.\n",
    "\n",
    "- However, a major weakness of this approach is the **very long training** and **testing** times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIVPlRxajoJW"
   },
   "source": [
    "#### Recurrent Neural Network Based Models\n",
    "\n",
    "\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/LM7.png\"width=\"390\" height=\"350\"/></center>\n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "- We have known that **feed-forward** neural network based LM use **fixed length** context. \n",
    "\n",
    "- However, recurrent neural network do not use **limited** size of **context**.\n",
    "\n",
    "- By using **recurrent** connections we can use **different** size of context.\n",
    "\n",
    "- The recurrent neural network based **language** model (RNNLM) provides further **generalization** by considering several **preceding words**. \n",
    "\n",
    "- A variant of **RNNLM** was presented to further **improve** the original RNNLM by **decreasing** its **computational complexity**, which was **implemented** by **factorization** of the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fo4lLLpWmJr"
   },
   "source": [
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/lm9.png\"height = \"300\"Width=\"700\"/></center>\n",
    "\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UGhDj0_SG1W"
   },
   "source": [
    " - The best models were the largest models, specifically number of memory units.\n",
    "\n",
    "- Use of **regularization** like dropout on **input connections** improves results.\n",
    "  \n",
    "- **Character-level** Convolutional Neural Network (CNN) models can be used on the **front-end** instead of word embeddings, achieving **similar** and sometimes **better** results.\n",
    "\n",
    "  - **Combining** the **prediction** from **multiple models** can offer large improvements in model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-8DBmYPZIh9"
   },
   "source": [
    "---\n",
    "<a name = Section4></a>\n",
    "# **4. Generalised Language Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isL3tq-tKtTd"
   },
   "source": [
    "- **Large-scale** pre-trained **language** modes like **OpenAI GPT** and **BERT** have achieved **great** performance on a **variety** of language tasks using **generic** model **architectures**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lbjvVuTY_uF"
   },
   "source": [
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/lm11.jpg\"height = \"400\"Width=\"700\"/></center>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2UzAfqMXGV0"
   },
   "source": [
    "#### **ELMO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jX9JUWy0uv7P"
   },
   "source": [
    "  - **ELMo**, short for **Embeddings from Language Model** learns **contextualized** word representation by pre-training a **language** model in an unsupervised way.\n",
    "\n",
    "- ELMo is applied on **semantic** intensive and **syntax intensive**.\n",
    "\n",
    "- **Semantic task:** The **word sense disambiguation** (WSD) task emphasizes the meaning of a **word** given a context. The **biLM** top layer is better at this task than the first layer.\n",
    "\n",
    "- **Syntax task:** The part-of-speech (POS) **tagging task** aims to infer the grammatical role of a **word** in one sentence. A **higher accuracy** can be achieved by using the biLM **first layer** than the top layer.\n",
    "\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/elmo.png\"height = \"350\"Width=\"700\"/></center>\n",
    "\n",
    "<br>  \n",
    "- A **residual connection** is added between the **first** and **second** LSTM layers. The input to the first layer is **added** to its **output** before being **passed** on as the **input** to the **second** layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtoeUf_NzHh5"
   },
   "source": [
    "### **GPT**\n",
    "\n",
    "OpenAI GPT, short for **Generative Pre-training Transformer**, expands the **unsupervised language** model to a much **larger** scale by training on a giant **collection** of free text corpora. Despite of the **similarity**, GPT has two major **differences** from ELMo.\n",
    "\n",
    "- The model architectures are different: ELMo uses a **shallow concatenation** of independently trained **left-to-right** and **right-to-left** multi-layer LSTMs, while GPT is a **multi-layer transformer decoder**.\n",
    "\n",
    "- The use of **contextualized embeddings** in downstream tasks are different: ELMo feeds **embeddings** into models **customized** for specific tasks as **additional features**, while GPT fine-tunes the same base **model** for all end tasks.\n",
    "\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/gpt.png\"width=\"750\" height=\"550\"/></center>\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okyoBurezXY3"
   },
   "source": [
    "### **BERT**\n",
    "\n",
    "- BERT, short for **Bidirectional Encoder Representations from Transformers** is a direct descendant to GPT: train a large language model on free text and then fine-tune on specific tasks without **customized** network **architectures**.\n",
    "\n",
    "- Compared to GPT, the **largest** difference and **improvement** of BERT is to make **training bi-directional**. The model learns to predict both context on the left and right. \n",
    "\n",
    "- To encourage the **bi-directional** prediction and **sentence-level** understanding, BERT is trained with **two tasks** instead of the **basic language** task.\n",
    "\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/bert.png\"width=\"850\" height=\"350\"/></center>\n",
    "\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvmLHkMhxS0s"
   },
   "source": [
    "#### **T5**\n",
    "\n",
    "- The language model T5 is short for **Text-to-Text Transfer Transformer**.\n",
    "\n",
    "- The **encoder-decoder** implementation follows the original Transformer architecture: \n",
    "\n",
    "   - tokens → embedding → encoder → decoder → output. \n",
    "   \n",
    "- Instead of an **explicit** QA format, T5 uses **short task prefixes** to distinguish **task intentions** and separately **fine-tunes** the model on every **individual** task.\n",
    "\n",
    "- The **text-to-text** framework **enables easier** transfer learning **evaluation** with the same model on a **diverse** set of tasks.\n",
    "\n",
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/T5.png\"height = \"300\"Width=\"700\"/></center>\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "radJl-Svz2v7"
   },
   "source": [
    "<br> \n",
    "<center><img src=\"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/s2s_data/sum.png\"width=\"850\" height=\"690\"/></center>\n",
    "\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnsMwjZFZIZL"
   },
   "source": [
    "---\n",
    "<a name = Section5></a>\n",
    "# **5. Application of Language Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWFN3K7YJKSV"
   },
   "source": [
    "#### Text Suggestions\n",
    "\n",
    "- **Google services** such as **Gmail** or **Google Docs** use language models to help **users** get text **suggestions** while they compose an email or create **long text** documents.  \n",
    "\n",
    "<br>  \n",
    "<center><img src =\" https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/MonstersEmail.0.gif\"height = \"300\"Width=\"600\"/></center>\n",
    "\n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVFmn0521pos"
   },
   "source": [
    "##  **Automatic Speech Recognition**\n",
    "\n",
    "- It is the technology that **allows human beings** to use their voices to **speak with a computer** interface.\n",
    "\n",
    "- It speaks in such a way that its most **sophisticated variations**, resembles normal **human conversation**.\n",
    "\n",
    "<br>  \n",
    "<center><img src =\"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/facetalk3.gif\"height = \"300\"Width=\"500\"/> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v0BvueD11zI"
   },
   "source": [
    "## **Machine Translation**\n",
    "\n",
    "- It is the task of **automatically converting** one natural language into another.\n",
    "\n",
    "- It **preserves** the **meaning** of the input text, and **producing fluent text** in the output language.\n",
    "\n",
    "<br>  \n",
    "<center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/1_zaK7KBzFtPFodu4CyauDYw.gif\"height = \"200\"Width=\"650\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BukH82ETZITI"
   },
   "source": [
    "---\n",
    "<a name = Section6></a>\n",
    "# **6. Conclusion**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoQ4peUPS_x2"
   },
   "source": [
    "- **Word embeddings** obtained through **NLMs** exhibit the **property** whereby **semantically** close words are likewise close in the **induced** vector space. \n",
    "\n",
    "- NLMs can also **capture** the **contextual information** at the **sentence-level**, corpus-level and **subword-level**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5dxto-TUPxdd"
   },
   "source": [
    "-  **Language modeling** is the art of determining the probability of a sequence of words.\n",
    "\n",
    "- This is useful in a large **variety** of areas **including** speech recognition, **optical character recognition**, **handwriting recognition**, **machine translation**, and **spelling correction**.\n",
    "\n",
    "- **Nonlinear** neural network models solve some of the **shortcomings** of **traditional** language models. \n",
    "\n",
    "- They allow **conditioning** on **increasingly** large **context sizes** with only a **linear** increase in the number of parameters, and they support generalization across **different contexts**.\n",
    "\n",
    "- That **state-of-the-art** results are achieved using **neural language models**, specifically those with **word embeddings** and recurrent neural network algorithms."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "01_Intro_to_Language_Modelling.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
